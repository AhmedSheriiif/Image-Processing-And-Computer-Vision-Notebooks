{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71dd26fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff14e8",
   "metadata": {},
   "source": [
    "## Hand Landmarks Mediapipe\n",
    "**Hand has 21 Point landmarks**\n",
    "<img src=\"Images/hand_pose.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a379ba",
   "metadata": {},
   "source": [
    "### Detecting Hands and Drawing Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28d75b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Calling mediapipe for hands\n",
    "mphands = mediapipe.solutions.hands\n",
    "\n",
    "# it takes the image flipped (as if it's selfie or front camera) --> we use cv2.flip(img,1)\n",
    "hands = mphands.Hands(static_image_mode = False, max_num_hands=2)\n",
    "\n",
    "# To draw connections between points\n",
    "mpDraw = mediapipe.solutions.drawing_utils\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        \n",
    "        # Mediapipe only works on RGB\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Applying Mediapipe to the images\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Drawing Circles and Connections when find a hand\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand in results.multi_hand_landmarks:\n",
    "                mpDraw.draw_landmarks(frame, hand, mphands.HAND_CONNECTIONS)\n",
    "        \n",
    "        \n",
    "                ## Printing Landmarks Position (for further Programming)\n",
    "                # for (idd, lm) in enumerate(hand.landmark):\n",
    "                    # print(idd, lm)\n",
    "                \n",
    "        # Showing the image\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        cv2.imshow('frame', frame)\n",
    "        \n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:  # ESC\n",
    "            break\n",
    "    else: break\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee2b85f",
   "metadata": {},
   "source": [
    "### Detecting Type of hand (left/right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c685a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Calling mediapipe for hands\n",
    "mphands = mediapipe.solutions.hands\n",
    "\n",
    "# it takes the image flipped (as if it's selfie or front camera)\n",
    "# if it's not the case use cv2.flip(img, 1)\n",
    "hands = mphands.Hands(static_image_mode = False, max_num_hands=2)\n",
    "\n",
    "# To draw connections between points\n",
    "mpDraw = mediapipe.solutions.drawing_utils\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        \n",
    "        handType = \"\"\n",
    "        # Mediapipe only works on RGB\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        \n",
    "        # Applying Mediapipe to the images\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Drawing Circles and Connections when find a hand\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand in results.multi_hand_landmarks:\n",
    "                mpDraw.draw_landmarks(frame, hand, mphands.HAND_CONNECTIONS)\n",
    "                \n",
    "            # HAND TYPE DETECTOR\n",
    "            for hand in results.multi_handedness:\n",
    "                handType=hand.classification[0].label\n",
    "        \n",
    "        \n",
    "        # Showing the image\n",
    "        frame = cv2.flip(frame,1)\n",
    "        cv2.putText(frame, handType, (10,20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 1)\n",
    "        \n",
    "        cv2.imshow('frame', frame)\n",
    "        \n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:  # ESC\n",
    "            break\n",
    "    else: break\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f600325",
   "metadata": {},
   "source": [
    "### Moving Mouse using Hand\n",
    "<img src=\"Images/hand_pose.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c1d477",
   "metadata": {},
   "source": [
    "#### Function to check which fingers are up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40e52020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use TIPs of Fingers --> Landmarks [4,8,12,16,20]\n",
    "# if all fingers are closed except (2nd Finger --> TIP[8]) then it moves the mouse\n",
    "# if all fingers are closed except (thumb finger --> TIP[4]) then it clicks!\n",
    "\n",
    "#### NOTE:\n",
    "# 1- To check if finger is up we compare using Y-axis\n",
    "# 2- To check if Thumbs is up we compare using X-axis\n",
    "\n",
    "def fingers(landmarks, handtype):\n",
    "    \"\"\"\n",
    "    landmarks: list of (id,position_x,position_y) for all landmarks\n",
    "    \"\"\"\n",
    "    fingerTips = []   # To store 5 sets of 1s or 0s\n",
    "    tipIds = [4,8,12,16,20]  # indexes for tips of each finger\n",
    "    \n",
    "    # Checking Thumbs UP (x-axis)\n",
    "    if handtype == 'Left':\n",
    "        if landmarks[tipIds[0]].x > landmarks[tipIds[0]-1].x:  # X landmark[4] > X landmark[3]  --> THUMBS\n",
    "            fingerTips.append(1)\n",
    "        else:\n",
    "            fingerTips.append(0)\n",
    "            \n",
    "    elif handtype == 'Right':\n",
    "        if landmarks[tipIds[0]].x < landmarks[tipIds[0]-1].x:  # X landmark[4] < X landmark[3]  --> THUMBS\n",
    "            fingerTips.append(1)\n",
    "        else:\n",
    "            fingerTips.append(0)\n",
    "        \n",
    "        \n",
    "    # check if all fingers are up --> [2] means y axis (in all other fingers we check y axis)\n",
    "    for id in range(1,5):\n",
    "        if landmarks[tipIds[id]].y < landmarks[tipIds[id] - 3].y:  # less than\n",
    "            fingerTips.append(1)\n",
    "        else:\n",
    "            fingerTips.append(0)\n",
    "            \n",
    "    return fingerTips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9818a6eb",
   "metadata": {},
   "source": [
    "#### Function to get position of finger on the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "64512957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_screen_position(finger, frame_h, frame_w):\n",
    "    \n",
    "    wScr, hScr = autopy.screen.size() # Width, Height of the screen\n",
    "\n",
    "    x1 = landmarks[4].x * frame_w\n",
    "    y1 = landmarks[4].y * frame_h\n",
    "        \n",
    "    x = np.interp(x1, (0,640), (0,wScr))  # converts width of the frame into screen width\n",
    "    y = np.interp(y1, (0,480), (0,hScr))  # converts height of the frame into screen height\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034bc2b",
   "metadata": {},
   "source": [
    "#### Collecting them, and show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "095bcd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To solve the problem of mouse vibration\n",
    "pX, pY = 0, 0   # prvious x and y locations  \n",
    "cX, cY = 0, 0   # current x and y locations\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Calling mediapipe for hands\n",
    "mphands = mediapipe.solutions.hands\n",
    "\n",
    "# if it's not the case use cv2.flip(img, 1)\n",
    "hands = mphands.Hands(static_image_mode = False, max_num_hands=1)\n",
    "\n",
    "# To draw connections between points\n",
    "mpDraw = mediapipe.solutions.drawing_utils\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    h, w, c = frame.shape\n",
    "    if ret == True:\n",
    "        \n",
    "        # Mediapipe only works on RGB\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Applying Mediapipe to the images\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Drawing Circles and Connections when find a hand\n",
    "        if results.multi_hand_landmarks:\n",
    "            # HAND TYPE DETECTOR\n",
    "            for hand in results.multi_handedness:\n",
    "                handType=hand.classification[0].label\n",
    "                \n",
    "            for hand in results.multi_hand_landmarks:\n",
    "                mpDraw.draw_landmarks(frame, hand, mphands.HAND_CONNECTIONS)\n",
    "            \n",
    "                # Getting Locations for each landmark\n",
    "                landmarks = results.multi_hand_landmarks[0].landmark\n",
    "                fingers_up = fingers(landmarks, handType)\n",
    "                \n",
    "                # CHECKING IF POINTER IS UP (MOVING THE MOUSE)\n",
    "                if fingers_up[1] == 1 and fingers_up[2] == 0 and fingers_up[3] == 0 and fingers_up[4] == 0:\n",
    "                    x, y = get_screen_position(landmarks[4], h, w)\n",
    "                    \n",
    "                    # to solve vibration of mouse\n",
    "                    cX = pX + (x - pX) / 7\n",
    "                    cY = pY + (y - pY) / 7\n",
    "                    pX, pY = cX, cY\n",
    "                    \n",
    "                    # moving mouse\n",
    "                    autopy.mouse.move(cX, cY)\n",
    "                \n",
    "                # CHECKING IF THUMBS IS ONLY UP (CLICK!)\n",
    "                elif fingers_up[0] == 1 and sum(fingers_up) == 1:\n",
    "                    autopy.mouse.click()  # left click\n",
    "                                     \n",
    "        \n",
    "        # Showing the image\n",
    "        frame = cv2.flip(frame,1)        \n",
    "        cv2.imshow('frame', frame)\n",
    "        \n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:  # ESC\n",
    "            break\n",
    "    else: break\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b3adc",
   "metadata": {},
   "source": [
    "## Pose Mediapipe Landmarks\n",
    "<img src=\"Images/pose_landmarks.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1d850c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe\n",
    "import numpy as np\n",
    "\n",
    "# Calling mediapipe for pose\n",
    "mp_drawing = mediapipe.solutions.drawing_utils\n",
    "mp_pose = mediapipe.solutions.pose\n",
    "\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "    \n",
    "    if ret == True:\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        results = pose.process(image)\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Render Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(255,0,0), thickness = 1))\n",
    "        \n",
    "        cv2.imshow('Mediapipe', image)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:  # ESC\n",
    "            break\n",
    "    else: break\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
